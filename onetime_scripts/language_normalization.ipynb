{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['»', 'That', '##sache', 'ist', ',', '«', 'rief', 'Don', 'Mel', '##chi', '##or', 'hoch', '##mü', '##th', '##ig', ',', '»', 'daß', 'Ihr', 'Euch', 'bei', 'mir', 'wie', 'ein', 'Miss', '##eth', '##äter', 'unter', 'einer', 'Verk', '##leidung', 'eingeführt', 'habt', ',', 'ohne', 'Zweifel', ',', 'um', 'mich', 'zu', 'ermor', '##den', '.', '«']\n",
      "[1665, 25477, 13886, 215, 818, 2529, 11344, 1703, 4404, 15305, 134, 2130, 10089, 1198, 214, 818, 1665, 848, 696, 8306, 282, 780, 335, 143, 4858, 3859, 979, 389, 369, 9127, 14977, 6816, 7422, 818, 1236, 9743, 818, 336, 804, 205, 11454, 169, 566, 2529]\n",
      "['»', 'That', '##sache', 'ist', ',', '«', 'rief', 'Don', 'Mel', '##chi', '##or', 'hoch', '##mü', '##th', '##ig', ',', '»', 'daß', 'Ihr', 'Euch', 'bei', 'mir', 'wie', 'ein', 'Miss', '##eth', '##äter', 'unter', 'einer', 'Verk', '##leidung', 'eingeführt', 'habt', ',', 'ohne', 'Zweifel', ',', 'um', 'mich', 'zu', 'ermor', '##den', '.', '«']\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"deepset/gbert-base\")\n",
    "tokens = tokenizer.tokenize(\n",
    "    \"»Thatsache ist,« rief Don Melchior hochmüthig, »daß Ihr Euch bei mir wie ein Missethäter unter einer Verkleidung eingeführt habt, ohne Zweifel, um mich zu ermorden.«\"\n",
    ")\n",
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(tokens)\n",
    "print(ids)\n",
    "print(tokenizer.convert_ids_to_tokens(ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ich', 'bin', 'in', 'der', 'That', 'bet', '##rü', '##bt', ',', 'Herr']\n",
      "[395, 1089, 153, 125, 25477, 657, 520, 451, 818, 1330]\n",
      "['Ich', 'bin', 'in', 'der', 'That', 'bet', '##rü', '##bt', ',', 'Herr']\n"
     ]
    }
   ],
   "source": [
    "tokens = tokenizer.tokenize(\"Ich bin in der That betrübt, Herr\")\n",
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(tokens)\n",
    "print(ids)\n",
    "print(tokenizer.convert_ids_to_tokens(ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Frau']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(\"Frau\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Der', 'jüngere', 'Tobias', 'auß', 'Befelch', 'deß', 'Engels', 'Raphaël', 'fangt', 'einen', 'Fisch', 'in', 'dem', 'Fluß', 'Tigris', ',', 'nimbt', 'auß', 'demselben', 'das', 'Hertz']\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 16, 17, 18, 19, 20]\n",
      "['Der' 'jüngere' 'Tobias' 'auß' 'Befelch' 'deß' 'Engels' 'Raphaël' 'fangt'\n",
      " 'einen' 'Fisch' 'in' 'dem' 'Fluß' 'Tigris' 'nimbt' 'auß' 'demselben'\n",
      " 'das' 'Hertz']\n",
      "[True, False, True, False, True, False, True, True, False, False, True, False, False, True, True, False, False, False, False, True]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "nltk.download(\"punkt\", quiet=True)\n",
    "\n",
    "# string = \"»Thatsache ist,« rief Don Melchior hochmüthig, »daß Ihr Euch bei mir wie ein Missethäter unter einer Verkleidung eingeführt habt, ohne Zweifel, um mich zu ermorden.«\"\n",
    "# string = \"Wosaken keem he darbi? Wo kunn 't angaan, dat se eren runden, weken Arm leggen dee in sinen knökerigen?\"\n",
    "# string = \"Dat weer in de provisorische Tiet von achtunveertig. All, wat man Been harr un en Flint dregen kunn, müß mit in den Krieg gegen de Dän. So waarn de Mannslüüd in Dörp un Stadt recht knapp, de dar nableben, weren Jungs tun öllerhafte oder ole Lüüd.\"\n",
    "# string = \"Ir sult sprechen willekomen: der iu mære bringet, daz bin ich.\"\n",
    "# string = \"Der Menschen müde Scharen Verlassen Feld und Werck wo Thir und Vögel waren Traurt itzt die Einsamkeit.\"\n",
    "string = \"Der jüngere Tobias auß Befelch deß Engels Raphaël fangt einen Fisch in dem Fluß Tigris, nimbt auß demselben das Hertz\"\n",
    "words = word_tokenize(string, language=\"german\")\n",
    "print(words)\n",
    "\n",
    "word_indices = [i for i in range(len(words)) if re.search(r\"\\w\", words[i]) is not None]\n",
    "print(word_indices)\n",
    "words_to_nomalize = np.array(words)[word_indices]\n",
    "capitalizations = [w[0].isupper() for w in words_to_nomalize]\n",
    "print(words_to_nomalize)\n",
    "print(capitalizations)\n",
    "with open(\"tmp.txt\", \"w\") as f:\n",
    "    f.writelines([w.lower() + \"\\n\" for w in words_to_nomalize])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "! docker run -v ~/university:/home mbollmann/norma -s -c norma/doc/example/example.cfg -f masters-thesis/onetime_scripts/tmp.txt > result.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Der', 'jüngere', 'Tobias', 'auß', 'Befelch', 'deß', 'Engels', 'Raphaël', 'fange', 'einen', 'Fisch', 'in', 'dem', 'Fluß', 'Tigris', ',', 'nimbt', 'auß', 'denselben', 'das', 'Herz']\n",
      "Der jüngere Tobias auß Befelch deß Engels Raphaël fange einen Fisch in dem Fluß Tigris , nimbt auß denselben das Herz\n",
      "Der jüngere Tobias auß Befelch deß Engels Raphaël fangt einen Fisch in dem Fluß Tigris, nimbt auß demselben das Hertz\n",
      "['Der', 'jüngere', 'Tobias', 'au', '##ß', 'Bef', '##el', '##ch', 'de', '##ß', 'Engels', 'Rap', '##ha', '##ë', '##l', 'fa', '##ng', '##e', 'einen', 'Fisch', 'in', 'dem', 'Flu', '##ß', 'Ti', '##gr', '##is', ',', 'ni', '##mb', '##t', 'au', '##ß', 'denselben', 'das', 'Herz']\n",
      "36\n",
      "['Der', 'jüngere', 'Tobias', 'au', '##ß', 'Bef', '##el', '##ch', 'de', '##ß', 'Engels', 'Rap', '##ha', '##ë', '##l', 'fa', '##ng', '##t', 'einen', 'Fisch', 'in', 'dem', 'Flu', '##ß', 'Ti', '##gr', '##is', ',', 'ni', '##mb', '##t', 'au', '##ß', 'demselben', 'das', 'Hert', '##z']\n",
      "37\n"
     ]
    }
   ],
   "source": [
    "with open(\"result.txt\", \"r\") as f:\n",
    "    lines = f.readlines()\n",
    "normalized_words = [l.strip().split(\"\\t\")[0] for l in lines]\n",
    "for i in range(len(normalized_words)):\n",
    "    if capitalizations[i]:\n",
    "        normalized_words[i] = normalized_words[i].capitalize()\n",
    "result = words.copy()\n",
    "for i in range(len(word_indices)):\n",
    "    to_replace_index = word_indices[i]\n",
    "    result[to_replace_index] = normalized_words[i]\n",
    "\n",
    "print(result)\n",
    "print(tokenizer.convert_tokens_to_string(result))\n",
    "print(string)\n",
    "norm_tokenized = tokenizer.tokenize(tokenizer.convert_tokens_to_string(result))\n",
    "print(norm_tokenized)\n",
    "print(len(norm_tokenized))\n",
    "print(tokenizer.tokenize(string))\n",
    "print(len(tokenizer.tokenize(string)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import List\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def tokenize_sequence(sequence: List, tokenizer: BertTokenizer) -> List:\n",
    "    p1, p2, is_continuation = sequence\n",
    "    p1 = tokenizer.convert_tokens_to_ids(tokenizer.tokenize(p1))\n",
    "    p2 = tokenizer.convert_tokens_to_ids(tokenizer.tokenize(p2))\n",
    "    return [p1[-254:], p2[:254], is_continuation]\n",
    "\n",
    "\n",
    "def book_to_data(\n",
    "    path: str, is_test: bool, tokenizer: BertTokenizer, csv_name, norm_fun = None\n",
    "):\n",
    "    df = pd.DataFrame(\n",
    "        {\n",
    "            \"p1_tokens\": [],\n",
    "            \"p2_tokens\": [],\n",
    "            \"is_continuation\": [],\n",
    "        }\n",
    "    )\n",
    "    if is_test:\n",
    "        df[\"book_path\"] = []\n",
    "        df[\"chapter_idx\"] = []\n",
    "        df[\"paragraph_idx\"] = []\n",
    "    df.to_csv(f\"{csv_name}.csv\", index=False, header=True, mode=\"w\")\n",
    "\n",
    "    with open(f\"../corpus/{path}\", \"r\", encoding=\"utf8\") as f:\n",
    "        book = json.load(f)\n",
    "\n",
    "    if norm_fun is not None:\n",
    "        book = norm_fun(book)\n",
    "\n",
    "    for chapter_index, chapter in enumerate(book[\"chapters\"]):\n",
    "        paragraphs = chapter[\"paragraphs\"]\n",
    "        for paragraph_index, paragraph in enumerate(paragraphs):\n",
    "            if chapter_index > 0 and paragraph_index == 0:\n",
    "                previous_paragraph = \" \".join(\n",
    "                    book[\"chapters\"][chapter_index - 1][\"paragraphs\"]\n",
    "                )\n",
    "                previous_paragraph = \" \".join(previous_paragraph.split(\" \")[-300:])\n",
    "                paragraph = \" \".join(paragraphs[paragraph_index : len(paragraphs)])\n",
    "                paragraph = \" \".join(paragraph.split(\" \")[:300])\n",
    "                sequence = tokenize_sequence(\n",
    "                    [\n",
    "                        previous_paragraph,\n",
    "                        paragraph,\n",
    "                        False,\n",
    "                    ],\n",
    "                    tokenizer,\n",
    "                )\n",
    "                df = pd.DataFrame(\n",
    "                    {\n",
    "                        \"p1_tokens\": [sequence[0]],\n",
    "                        \"p2_tokens\": [sequence[1]],\n",
    "                        \"is_continuation\": [sequence[2]],\n",
    "                    }\n",
    "                )\n",
    "                if is_test:\n",
    "                    df[\"book_path\"] = path\n",
    "                    df[\"chapter_idx\"] = chapter_index\n",
    "                    df[\"paragraph_idx\"] = paragraph_index\n",
    "                df.to_csv(f\"{csv_name}.csv\", index=False, header=False, mode=\"a\")\n",
    "\n",
    "            elif paragraph_index > 0:\n",
    "                previous_paragraph = \" \".join(paragraphs[0:paragraph_index])\n",
    "                previous_paragraph = \" \".join(previous_paragraph.split(\" \")[-300:])\n",
    "                paragraph = \" \".join(paragraphs[paragraph_index : len(paragraphs)])\n",
    "                paragraph = \" \".join(paragraph.split(\" \")[:300])\n",
    "                sequence = tokenize_sequence(\n",
    "                    [previous_paragraph, paragraph, True], tokenizer\n",
    "                )\n",
    "                df = pd.DataFrame(\n",
    "                    {\n",
    "                        \"p1_tokens\": [sequence[0]],\n",
    "                        \"p2_tokens\": [sequence[1]],\n",
    "                        \"is_continuation\": [sequence[2]],\n",
    "                    }\n",
    "                )\n",
    "                if is_test:\n",
    "                    df[\"book_path\"] = path\n",
    "                    df[\"chapter_idx\"] = chapter_index\n",
    "                    df[\"paragraph_idx\"] = paragraph_index\n",
    "                df.to_csv(f\"{csv_name}.csv\", index=False, header=False, mode=\"a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "import numpy as np\n",
    "import subprocess\n",
    "import os\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "\n",
    "detokenizer = TreebankWordDetokenizer()\n",
    "\n",
    "nltk.download(\"punkt\", quiet=True)\n",
    "\n",
    "\n",
    "def norma(book: dict):\n",
    "    chapters = book[\"chapters\"]\n",
    "    new_chapters = []\n",
    "    for chapter in tqdm(chapters):\n",
    "        new_chapter = {\"name\": chapter[\"name\"], \"idx\": chapter[\"idx\"]}\n",
    "        new_paragraphs = []\n",
    "        with open(\"tmp.txt\", \"w\") as f:\n",
    "            f.write(\"\")\n",
    "        paragraph_info = []\n",
    "        for paragraph in chapter[\"paragraphs\"]:\n",
    "            words = word_tokenize(paragraph, language=\"german\")\n",
    "            word_indices = [\n",
    "                i for i in range(len(words)) if re.search(r\"\\w\", words[i]) is not None\n",
    "            ]\n",
    "            words_to_normalize = np.array(words)[word_indices]\n",
    "            capitalizations = [w[0].isupper() for w in words_to_normalize]\n",
    "            paragraph_info.append({\"words\": words, \"word_indices\": word_indices, \"capitalizations\": capitalizations})\n",
    "            with open(\"tmp.txt\", \"a\") as f:\n",
    "                f.writelines([w.lower() + \"\\n\" for w in words_to_normalize])\n",
    "                f.write(\"_____PARAGRAPH_____\\n\")\n",
    "\n",
    "        os.system(\n",
    "            \"docker run -v ~/university:/home mbollmann/norma -s -c norma/doc/example/example.cfg -f masters-thesis/onetime_scripts/tmp.txt > result.txt\"\n",
    "        )\n",
    "\n",
    "        with open(\"result.txt\", \"r\") as f:\n",
    "            lines = f.readlines()\n",
    "        \n",
    "        paragraph_lists = []\n",
    "        tmp_list = []\n",
    "        for line in lines:\n",
    "            if line != \"_____PARAGRAPH_____\\t0\\n\":\n",
    "                tmp_list.append(line)\n",
    "            else:\n",
    "                paragraph_lists.append(tmp_list)\n",
    "                tmp_list = []\n",
    "        for index, lines in enumerate(paragraph_lists):\n",
    "            words = paragraph_info[index][\"words\"]\n",
    "            word_indices = paragraph_info[index][\"word_indices\"]\n",
    "            capitalizations = paragraph_info[index][\"capitalizations\"]\n",
    "            normalized_words = [l.strip().split(\"\\t\")[0] for l in lines]\n",
    "            for i in range(len(normalized_words)):\n",
    "                if capitalizations[i]:\n",
    "                    normalized_words[i] = normalized_words[i].capitalize()\n",
    "            result = words.copy()\n",
    "            for i in range(len(word_indices)):\n",
    "                to_replace_index = word_indices[i]\n",
    "                result[to_replace_index] = normalized_words[i]\n",
    "            new_paragraphs.append(detokenizer.detokenize(result))\n",
    "        new_chapter[\"paragraphs\"] = new_paragraphs\n",
    "        new_chapters.append(new_chapter)\n",
    "    new_book = book.copy()\n",
    "    new_book[\"chapters\"] = new_chapters\n",
    "    with open(\"tmp_unnorm.json\", \"w\") as f:\n",
    "        json.dump(book, f, ensure_ascii=False)\n",
    "    with open(\"tmp_norm.json\", \"w\") as f:\n",
    "        json.dump(new_book, f, ensure_ascii=False)\n",
    "    return new_book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [00:37<00:00,  3.09s/it]\n"
     ]
    }
   ],
   "source": [
    "book_to_data(\"fehrs_maren.json\", True, tokenizer, \"fehrs_maren\")\n",
    "book_to_data(\"fehrs_maren.json\", True, tokenizer, \"fehrs_maren_norma_nochain\", norma)\n",
    "book_to_data(\"abraham_narrnest.json\", True, tokenizer, \"abraham_narrnest\")\n",
    "book_to_data(\"abraham_narrnest.json\", True, tokenizer, \"abraham_narrnest_norma_nochain\", norma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "def preprocess(example, tokenizer):\n",
    "    p1_tokens = list(map(json.loads, example[\"p1_tokens\"]))\n",
    "    p2_tokens = list(map(json.loads, example[\"p2_tokens\"]))\n",
    "    sequences = list(zip(p1_tokens, p2_tokens))\n",
    "    labels = example[\"is_continuation\"]\n",
    "    batch_encoding = tokenizer.batch_encode_plus(\n",
    "        sequences,\n",
    "        add_special_tokens=True,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        return_tensors=\"pt\",\n",
    "        return_token_type_ids=True,\n",
    "        return_attention_mask=True,\n",
    "    )\n",
    "\n",
    "    output = batch_encoding\n",
    "    output[\"book_path\"] = example[\"book_path\"]\n",
    "    output[\"chapter_idx\"] = example[\"chapter_idx\"]\n",
    "    output[\"paragraph_idx\"] = example[\"paragraph_idx\"]\n",
    "    output[\"labels\"] = torch.tensor(labels, dtype=torch.uint8)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration .-cf495a19cf2bf033\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset csv/. to /home/sylvarus/.cache/huggingface/datasets/csv/.-cf495a19cf2bf033/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data files: 100%|██████████| 1/1 [00:00<00:00, 1566.21it/s]\n",
      "Extracting data files: 100%|██████████| 1/1 [00:00<00:00, 1038.71it/s]\n",
      "                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset csv downloaded and prepared to /home/sylvarus/.cache/huggingface/datasets/csv/.-cf495a19cf2bf033/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317. Subsequent calls will reuse this data.\n",
      "Dataset({\n",
      "    features: ['p1_tokens', 'p2_tokens', 'is_continuation', 'book_path', 'chapter_idx', 'paragraph_idx'],\n",
      "    num_rows: 2938\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 734/735 [00:01<00:00, 634.54ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2938\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 735/735 [42:29<00:00,  3.47s/it]\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForNextSentencePrediction\n",
    "import datasets\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "results = []\n",
    "BATCH_SIZE = 4\n",
    "\n",
    "# Model and device setup\n",
    "if torch.cuda.device_count() > 0:\n",
    "    print(\n",
    "        f\"Devices available: {torch.cuda.device_count()}. Device 0: {torch.cuda.get_device_name(0)}.\"\n",
    "    )\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "auth_token = input(\"Enter auth token: \").strip()\n",
    "tokenizer = BertTokenizer.from_pretrained(\"deepset/gbert-base\")\n",
    "\n",
    "model = BertForNextSentencePrediction.from_pretrained(\n",
    "    \"MzudemO/chapter-segmentation-model\", use_auth_token=auth_token\n",
    ")\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "# Dataset setup\n",
    "dataset = datasets.load_dataset(\n",
    "    \"./\",\n",
    "    data_files={\"dataset\": \"temp_df.csv\"},\n",
    "    split=\"dataset\",\n",
    ")\n",
    "\n",
    "print(dataset)\n",
    "\n",
    "dataset = dataset.map(\n",
    "    lambda example: preprocess(example, tokenizer),\n",
    "    batched=True,\n",
    "    batch_size=BATCH_SIZE,\n",
    ")\n",
    "dataset = dataset.with_format(\n",
    "    \"torch\",\n",
    "    columns=[\n",
    "        \"input_ids\",\n",
    "        \"token_type_ids\",\n",
    "        \"attention_mask\",\n",
    "        \"labels\",\n",
    "        \"book_path\",\n",
    "        \"chapter_idx\",\n",
    "        \"paragraph_idx\",\n",
    "    ],\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "print(len(dataset))\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "for batch in tqdm(dataloader):\n",
    "    model_batch = {\n",
    "        k: v.to(device)\n",
    "        for k, v in batch.items()\n",
    "        if k in [\"input_ids\", \"token_type_ids\", \"attention_mask\", \"labels\"]\n",
    "    }\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**model_batch)\n",
    "\n",
    "    logits = outputs.logits\n",
    "\n",
    "    for index, logit in enumerate(logits):\n",
    "        results.append(\n",
    "            [\n",
    "                batch[\"book_path\"][index],\n",
    "                batch[\"chapter_idx\"][index].cpu().item(),\n",
    "                batch[\"paragraph_idx\"][index].cpu().item(),\n",
    "                batch[\"labels\"][index].cpu().item(),\n",
    "                logit[0].cpu().item(),\n",
    "                logit[1].cpu().item(),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "df = pd.DataFrame(results)\n",
    "df = df.rename(\n",
    "    columns={\n",
    "        0: \"book_path\",\n",
    "        1: \"chapter_idx\",\n",
    "        2: \"paragraph_idx\",\n",
    "        3: \"labels\",\n",
    "        4: \"logit_0\",\n",
    "        5: \"logit_1\",\n",
    "    }\n",
    ")\n",
    "df.to_pickle(\"temp_predictions_fehrs_maren.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "msc-thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
